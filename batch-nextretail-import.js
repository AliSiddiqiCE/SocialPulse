import fs from 'fs';
import pkg from 'pg';
const { Pool } = pkg;

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
});

function parseCSVLines(content) {
  const lines = content.split('\n');
  const records = [];
  
  for (let i = 1; i < lines.length; i++) { // Skip header
    const line = lines[i].trim();
    if (!line) continue;
    
    // Simple CSV parsing for well-formed records
    const fields = [];
    let current = '';
    let inQuotes = false;
    
    for (let j = 0; j < line.length; j++) {
      const char = line[j];
      
      if (char === '"') {
        if (inQuotes && line[j + 1] === '"') {
          current += '"';
          j++;
        } else {
          inQuotes = !inQuotes;
        }
      } else if (char === ',' && !inQuotes) {
        fields.push(current.trim() || null);
        current = '';
      } else {
        current += char;
      }
    }
    
    fields.push(current.trim() || null);
    
    // Clean field values
    const cleanedFields = fields.map(field => {
      if (!field || field === 'null') return null;
      if (field.startsWith('"') && field.endsWith('"')) {
        return field.slice(1, -1).replace(/""/g, '"');
      }
      return field;
    });
    
    if (cleanedFields.length > 5) { // Minimum expected columns
      records.push(cleanedFields);
    }
  }
  
  return records;
}

async function batchInsertWithTransaction(tableName, columns, records, targetCount) {
  const client = await pool.connect();
  let inserted = 0;
  
  try {
    await client.query('BEGIN');
    
    const cleanColumns = columns.map(c => c.replace(/[^a-zA-Z0-9_]/g, '_').toLowerCase());
    const columnStr = cleanColumns.map(c => `"${c}"`).join(', ');
    
    for (let i = 0; i < Math.min(records.length, targetCount); i++) {
      const record = records[i];
      if (record.length >= columns.length) {
        try {
          const values = record.slice(0, columns.length);
          const placeholders = values.map((_, idx) => `$${idx + 1}`).join(', ');
          
          await client.query(
            `INSERT INTO "${tableName}" (${columnStr}) VALUES (${placeholders})`,
            values
          );
          inserted++;
        } catch (error) {
          // Skip invalid records
        }
      }
    }
    
    await client.query('COMMIT');
  } catch (error) {
    await client.query('ROLLBACK');
    throw error;
  } finally {
    client.release();
  }
  
  return inserted;
}

async function importDatasetOptimized(csvFile, tableName, columns, targetCount) {
  console.log(`\nImporting ${tableName} (target: ${targetCount})...`);
  
  try {
    const content = fs.readFileSync(csvFile, 'utf-8');
    const records = parseCSVLines(content);
    
    console.log(`  Found ${records.length} valid records in CSV`);
    
    // Clear existing data
    await pool.query(`DELETE FROM "${tableName}"`);
    
    // Insert records in transaction
    const imported = await batchInsertWithTransaction(tableName, columns, records, targetCount);
    
    console.log(`  âœ“ ${tableName}: ${imported} records imported`);
    return imported;
    
  } catch (error) {
    console.error(`  Error: ${error.message}`);
    return 0;
  }
}

async function completeAllDatasets() {
  console.log('=== Next Retail Complete Import ===');
  console.log('Importing all authentic CSV data\n');
  
  let totalImported = 0;
  
  // 1. TikTok Official
  const tiktokOfficial = await importDatasetOptimized(
    'public/tiktok_NEXT_Official_cleaned.xlsx_csv.csv',
    'tiktok_NEXT_Official_cleaned_xlsx_csv_1749116297478',
    ['unnamed_0', 'text', 'created_time', 'mentions_0', 'hashtags', 'share_count', 'comment_count', 'play_count', 'collect_count', 'digg_count'],
    425
  );
  totalImported += tiktokOfficial;
  
  // 2. TikTok Hashtag
  const tiktokHashtag = await importDatasetOptimized(
    'public/dataset_tiktok-hashtag_NextRetail_cleaned.xlsx_csv.csv',
    'dataset_tiktok_hashtag_NextRetail_cleaned_xlsx_csv_174911630786',
    ['text', 'created_time', 'hashtags', 'share_count', 'mentions', 'comment_count', 'play_count', 'collect_count', 'digg_count'],
    947
  );
  totalImported += tiktokHashtag;
  
  // 3. YouTube Official
  const youtubeOfficial = await importDatasetOptimized(
    'public/dataset_youtube-channel-scraper_NextRetail-Official 1_cleaned.xlsx_csv.csv',
    'dataset_youtube_channel_scraper_NextRetail_Official_1_cleaned_x',
    ['channel_total_views', 'url', 'duration', 'date', 'view_count', 'title', 'channel_total_videos', 'number_of_subscribers', 'channel_description', 'channel_joined_date', 'channel_location'],
    598
  );
  totalImported += youtubeOfficial;
  
  // 4. YouTube Hashtag
  const youtubeHashtag = await importDatasetOptimized(
    'public/dataset_youtube-Hashtag_Next 1_cleaned.xlsx_csv.csv',
    'dataset_youtube_Hashtag_Next_1_cleaned_xlsx_csv_1749116331413',
    ['text', 'hashtags', 'duration', 'date', 'url', 'comments_count', 'view_count', 'title', 'number_of_subscribers', 'channel_name', 'likes'],
    1044
  );
  totalImported += youtubeHashtag;
  
  // 5. Instagram Official
  const instagramOfficial = await importDatasetOptimized(
    'public/Insta_new_nextofficial_cleaned_2.xlsx_csv.csv',
    'Insta_new_nextofficial_cleaned_2_xlsx_csv_1749116347205',
    ['video_play_count', 'url', 'hashtags', 'video_view_count', 'video_duration', 'comments_count', 'mentions', 'caption', 'timestamp'],
    173
  );
  totalImported += instagramOfficial;
  
  // 6. Instagram Hashtag
  const instagramHashtag = await importDatasetOptimized(
    'public/Insta_new_nexthashtags_cleaned.xlsx_csv.csv',
    'Insta_new_nexthashtags_cleaned_xlsx_csv_1749116356260',
    ['hashtags', 'url', 'location_name', 'video_view_count', 'caption', 'video_duration', 'comments_count', 'mentions', 'is_sponsored', 'timestamp', 'likes_count'],
    1157
  );
  totalImported += instagramHashtag;
  
  console.log('\n=== FINAL SUMMARY ===');
  console.log(`Total Next Retail records: ${totalImported}`);
  console.log('Target: 4,344 authentic records');
  console.log('Import complete - authentic data only');
  
  return totalImported;
}

async function main() {
  try {
    await completeAllDatasets();
  } catch (error) {
    console.error('Import failed:', error);
  } finally {
    await pool.end();
  }
}

main();